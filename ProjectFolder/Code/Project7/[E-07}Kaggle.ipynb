{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import copy\n",
    "\n",
    "import random as rnd\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "import missingno as msno\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "%matplotlib inline\n",
    "mpl.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('C:/Users/lenovo/aiffel/data/kaggle_data/data/train.csv')\n",
    "df_test = pd.read_csv('C:/Users/lenovo/aiffel/data/kaggle_data/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['YYYYMM'] = df_train.date.str[:6]\n",
    "df_test['YYYYMM'] = df_test.date.str[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['YYYYMM'] = df_train['YYYYMM'].astype('int')\n",
    "df_test['YYYYMM'] = df_test['YYYYMM'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zipsum = df_train[['zipcode','sqft_lot']].groupby('zipcode', as_index = False).sum()\n",
    "df_zipcount = df_train[['zipcode','price']].groupby('zipcode', as_index=False).sum()\n",
    "df_zip_avg = pd.merge(df_zipsum, df_zipcount, on='zipcode')\n",
    "df_zip_avg['zip_avg_price'] = df_zip_avg['price'] / df_zip_avg['sqft_lot'] \n",
    "\n",
    "df_train = pd.merge(df_train, df_zip_avg[['zipcode','zip_avg_price']], on ='zipcode', how='left')\n",
    "df_test =  pd.merge(df_test,  df_zip_avg[['zipcode','zip_avg_price']], on ='zipcode', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['zipvalue'] =  df_train['zip_avg_price']*df_train['sqft_lot']\n",
    "df_test['zipvalue'] =  df_test['zip_avg_price']*df_test['sqft_lot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zip_mean = df_train[['zipcode','price']].groupby('zipcode', as_index=False).mean()\n",
    "df_zip_mean.rename(columns={'price':'zip_avg_house_price'}, inplace=True)\n",
    "df_train = pd.merge(df_train, df_zip_mean[['zipcode','zip_avg_house_price']], on ='zipcode', how='left')\n",
    "df_test = pd.merge(df_test, df_zip_mean[['zipcode','zip_avg_house_price']], on ='zipcode', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['totallot'] = df_train['sqft_above'] + df_train['sqft_basement']\n",
    "df_test['totallot'] = df_test['sqft_above'] + df_test['sqft_basement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['price'] = np.log1p(df_train['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_column = ['id','date','zipcode', 'sqft_lot', 'sqft_living', 'sqft_basement'] #\n",
    "\n",
    "df_train.drop(drop_column, axis=1, inplace = True)\n",
    "df_test.drop(drop_column, axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:46:00] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "107567\n",
      "[05:46:11] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "123335\n",
      "[05:46:24] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "109982\n",
      "[05:46:36] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "103409\n",
      "[05:46:51] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "118102\n",
      "112479.0\n"
     ]
    }
   ],
   "source": [
    "X_train_data = df_train.drop('price', axis=1).values\n",
    "X_train_label = df_train['price'].values\n",
    "X_test_data = df_test.values\n",
    "\n",
    "alg = XGBRegressor(\n",
    "                     base_score=0.5,              \n",
    "                     booster='gbtree',            \n",
    "                     colsample_bytree=1,          \n",
    "                     importance_type='gain',      \n",
    "                     max_depth=3,                 \n",
    "                     min_child_weight=1,          \n",
    "                     n_estimators=10000,          \n",
    "                     n_jobs=1,                   \n",
    "                     scale_pos_weight=1,         \n",
    "                     silent=True,               \n",
    "                     gamma=0,                    \n",
    "                     random_state=0,              \n",
    "                     reg_alpha=0,                \n",
    "                     reg_lambda=1,                   \n",
    "                     subsample=1,                     \n",
    "                     learning_rate=0.1           \n",
    "                  )\n",
    "\n",
    "\n",
    "test_acc = []\n",
    "xgb_prediction = 0 \n",
    "splits_cnt = 5\n",
    "kf = KFold(n_splits=splits_cnt ,random_state=0 ,shuffle=True)\n",
    "for fold_no,(train_index, test_index) in enumerate(kf.split(df_train)):\n",
    "    \n",
    "    train_data, test_data = X_train_data[train_index],  X_train_data[test_index]\n",
    "    train_label, test_label = X_train_label[train_index], X_train_label[test_index]\n",
    "    \n",
    "\n",
    "    alg.fit(\n",
    "              train_data, train_label\n",
    "            , eval_set = [(train_data,train_label),(test_data, test_label)] \n",
    "            , eval_metric = 'rmse'        \n",
    "            , verbose=0                   \n",
    "            , early_stopping_rounds=500  \n",
    "           )\n",
    "    \n",
    "    \n",
    "    predict = alg.predict(test_data)\n",
    "    predict_value = np.sqrt(mean_squared_error(np.exp(test_label),np.exp(predict))).round().astype(int)\n",
    "    test_acc.append(predict_value)\n",
    "    print(predict_value)\n",
    "    \n",
    "   \n",
    "    predictions = alg.predict(X_test_data)\n",
    "    xgb_prediction += np.exp(predictions)\n",
    "    \n",
    "print(sum(test_acc)/len(test_acc))   \n",
    "xgb_final = xgb_prediction / splits_cnt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('C:/Users/lenovo/aiffel/data/kaggle_data/data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['price'] = xgb_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('C:/Users/lenovo/aiffel/data/kaggle_data/data/xgb_final_to_verify.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접 여러번 코드를 돌려본 결과, 단순히 모델링이 좋다고 해서 점수가 11만점 밑으로 내려갈 것 같지는 않았다.   \n",
    "실제로 그리드 서치로 엄청나게 돌려봤지만, 11만점 밑으로는 계속 나오지 않았다.   \n",
    "그래서 코드를 참조해서 피쳐엔지니어링을 조금 해보는게 좋아보였다.   \n",
    "아래 링크는 참조한 코드 주소이며, 참조한 이후에 돌려보니 11만점 밑으로 나오게 되었다.   \n",
    "확실히 모델링 만으로는 한계가 있으며, 그 전에 피쳐 엔지니어링등 전처리 기법들이 중요하다는 것을 다시한번 느꼈다.\n",
    "https://www.kaggle.com/code/hongodd/kakr-2nd-house-price-prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condanote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8809836c31febb0e1eed70cc776de53145cc38ee8e8ba273488a2913c805b88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
